# Issue #6: Add DB table for scrape session records

**Started**: 2026-01-21 14:31 UTC
**Issue**: https://github.com/shotleybuilder/sertantai-legal/issues/6

**Checkpoint**: `git reset --hard pre-issue-6-refactor`

## Todo

### Phase 1: Session Records Table - COMPLETE
- [x] Create `ScrapeSessionRecord` Ash resource with unique constraint
- [x] Generate and run migration (`20260121130840_add_scrape_session_records`)
- [x] Update `Storage` module to write session records to DB
- [x] Update `ScrapeController` to query session records from DB
- [x] Keep JSON read fallback for backwards compatibility
- **Commit**: `4d3c4d1`

### Phase 2: Cascade Affected Laws Table - COMPLETE
- [x] Create `CascadeAffectedLaw` Ash resource with unique `(session_id, affected_law)` constraint
- [x] Generate and run migration (`20260121132833_add_cascade_affected_laws`)
- [x] Update `Storage.add_affected_laws/5` to upsert to DB (append to source_laws array)
- [x] Update `get_affected_laws_summary` to query from DB
- [x] Update cascade endpoints (`batch_reparse`, `update_enacting_links`) to mark entries as processed
- [x] Update `clear_affected_laws` to delete from DB
- [x] Keep JSON fallback for backwards compatibility
- **Commit**: `6b0dc23`

### Phase 2b: Fix parsed_data Storage - COMPLETE
- [x] **BUG**: `save_session_record/3` only stores `law_name`, not full scrape metadata
- [x] Update `save_session_record/3` to store full record data in `parsed_data`
- [x] The `parsed_data` field should contain all scrape metadata (title, year, type, etc.)
- [x] Parser adds to `parsed_data` on second parse (amendments, etc.) - builds up over parses
- [x] Verify `session_record_to_map/1` correctly returns full data from `parsed_data`
- **Fix**: Added `record_to_parsed_data/1` helper in `storage.ex`

### Phase 3: Migration & Cleanup
- [x] Create migration script to migrate 8 existing scrape sessions to new DB tables
  - Script: `scripts/data/migrate_sessions_to_db.exs`
  - Supports `--dry-run` and `--session=<id>` options
  - Migrated 1,287 session records and 187 cascade entries
- [x] Test full workflow end-to-end with migrated data (522 tests pass)
- [x] Decision: KEEP dual-write (DB + JSON) for backup/audit

### Phase 3b: Fix Migration Script - COMPLETE
- [x] **BUG**: Migration script only copied `law_name`, not full JSON record data
- [x] Update migration script to populate `parsed_data` with full record from JSON
- [x] Re-run migration for existing sessions to backfill `parsed_data`
- [x] Verified DB now has full metadata (title, year, number, etc.)
- **Fix**: Added `record_to_parsed_data/1` helper in migration script
- **Re-migrated**: 1,287 session records with full data

### Phase 4: Unit Tests for New Resources - COMPLETE
- [x] Add unit tests for `ScrapeSessionRecord` Ash resource (26 tests)
  - Test create with required/optional fields
  - Test validation of group and status constraints
  - Test unique constraint upsert behavior
  - Test status transitions (mark_parsed, mark_confirmed, mark_skipped)
  - Test selection toggle and queries
  - Test query actions (by_session, by_session_and_group, etc.)
  - Test edge cases (empty/complex parsed_data, special characters)
- [x] Add unit tests for `CascadeAffectedLaw` Ash resource (26 tests)
  - Test create with required/optional fields
  - Test validation of update_type and status constraints
  - Test unique constraint behavior
  - Test append_source_law (add new, skip duplicates, sequence)
  - Test upgrade_to_reparse transition
  - Test mark_processed status change
  - Test query actions (by_session, by_session_and_type, etc.)
  - Test full workflow (create -> append -> upgrade -> process)
  - Test deduplication scenario simulation
- **Commit**: `bb1fcc0`
- **Total**: 574 tests (522 existing + 52 new), 0 failures

### Phase 5: UI Enhancements (IN PROGRESS)

**Problem**: User has no visibility into whether data is served from DB or JSON files. The fallback read strategy is silent.

**Proposed UI Enhancements:**

- [x] **Data source indicator** on session detail page - **Commit**: `e5cc36a`
  - Show badge: "DB" (green) or "JSON (legacy)" (yellow)
  - API returns `data_source: "db" | "json"` in response
  - Files: `storage.ex`, `scrape_controller.ex`, `scraper.ts`, `+page.svelte`
  
- [SKIPPED] **Record-level status column** in session tables
  - Show status: pending → parsed → confirmed/skipped
  - Color-coded: pending (gray), parsed (blue), confirmed (green), skipped (red)
  - DB field: `scrape_session_records.status`
  - **SKIPPED REASON**: Investigation found `mark_parsed` is never called in current flow.
    Parsing happens client-side (browser), server only knows about confirmation.
    Records go directly `pending` → `confirmed`, so `parsed` status is unused.
    Would require API changes to notify server when parsing completes before confirmation.
    Limited utility until server-side parse tracking is implemented.

- [SKIPPED] **Parse count indicator** on records that have been re-parsed
  - Show badge with count if > 1 (e.g., "×2" for twice-parsed)
  - DB field: `scrape_session_records.parse_count`
  - **SKIPPED REASON**: Per-session parse count has limited value. Lifetime parse count
    across all sessions would be more useful. Created Issue #8 to track adding
    `total_parse_count` to `uk_lrt` table as future enhancement.

- [x] **Cascade update progress** on session detail page - **Commit**: `c94f459`
  - Show: "Cascade: 5 pending, 3 processed" 
  - Link to cascade panel with breakdown by update_type
  - DB fields: `cascade_affected_laws.status`, `cascade_affected_laws.update_type`
  - **BUG FIXED**: Cascade Update button now shows based on pending entries in `cascade_affected_laws` table
    (was incorrectly using `session.persisted_count > 0`)
  - Implementation:
    1. Added `pending_count` and `processed_count` to affected_laws API response
    2. Button shows when `cascadePendingCount > 0 || cascadeProcessedCount > 0`
    3. Display count badge on button: "Cascade Update (5 pending)"
    4. Gray button when all processed, shows completion status

- [SKIPPED] **Timestamps** on records
  - Show "Added: 2h ago" or hover tooltip with full timestamp
  - DB fields: `inserted_at`, `updated_at`
  - **SKIPPED REASON**: Low utility - timestamps don't provide actionable information during review workflow.

- [x] **Selection persistence** - ALREADY WORKING
  - Selected state should survive page refresh
  - DB field: `scrape_session_records.selected`
  - **VERIFIED**: Backend already returns `selected` from DB in `session_record_to_map/1`
  - API endpoint returns selection state, frontend displays it correctly
  - Updates write to both JSON and DB via `update_selection_db`

- [DEFERRED TO CASCADE MODAL] **Batch cascade update button**
  - Currently: cascade updates run synchronously during each parse
  - Desired: Option to defer cascade updates until all parsing complete
  - **DEFERRED REASON**: Batch functionality already exists in `CascadeUpdateModal.svelte`
    via `batchReparse()`. Adding a separate button on session page would mix contexts
    (new law parsing vs cascade updates). Any batch improvements should be made in
    the cascade modal, not the session detail page.

**Implementation Notes:**
- Backend: Add `data_source` field to API responses
- Frontend: Update session detail components to display new fields
- Consider: Migration indicator for sessions with JSON-only data

---

### Recommendation: Keep JSON Files

**Decision**: Keep the dual-write approach (DB + JSON) for production.

**Rationale**:

1. **VPS Deployment Compatibility**: JSON files work fine on a VPS with persistent disk. The `priv/scraper/` directory can be mounted as a persistent volume or simply exist on the VPS filesystem.

2. **Human-Readable Audit Trail**: JSON files provide an easily inspectable audit trail without needing DB queries. Useful for debugging scrape sessions directly on the server.

3. **Backup/Recovery**: If the DB gets corrupted or needs to be rebuilt, the JSON files serve as a source of truth for re-migration using the existing script.

4. **Low Overhead**: Writing JSON files is cheap - the files are small and writes are infrequent (only during scrape sessions).

5. **Graceful Degradation**: If DB writes fail for any reason, the JSON files ensure no data loss.

**Current Architecture** (recommended to keep):
```
WRITES (dual-write on every scrape):
  Parse Law → Write to DB + Write to JSON (both always triggered)

READS (DB-first with fallback):
  Query DB first → Fall back to JSON if DB empty (backwards compat)
```

**Future Consideration**: If disk space becomes a concern or JSON files cause issues, they can be removed by:
1. Removing JSON write calls from `Storage` module
2. Removing JSON fallback reads (DB becomes sole source)
3. Archiving/deleting old JSON session directories

## Notes
- **Two separate concerns**:
  1. Session records = laws being parsed in this session (groups 1/2/3)
  2. Cascade affected laws = todo list of OTHER laws needing update

---

## Schema Design

### Table 1: `scrape_session_records` (Laws Being Parsed) - IMPLEMENTED

Tracks each law being processed in a scrape session.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         scrape_session_records                               │
├──────────────────────────┬──────────────────────┬────────────────────────────┤
│ Column                   │ Type                 │ Notes                      │
├──────────────────────────┼──────────────────────┼────────────────────────────┤
│ id                       │ uuid (PK)            │                            │
│ session_id               │ string               │ -> scrape_sessions.session_id │
│ law_name                 │ string               │ e.g. "UK_uksi_2025_622"    │
│ group                    │ atom                 │ :group1, :group2, :group3  │
│ status                   │ atom                 │ :pending, :parsed, :confirmed, :skipped │
│ selected                 │ boolean              │ For bulk selection UI      │
│ parsed_data              │ map (JSONB)          │ Full ParsedLaw output      │
│ parse_count              │ integer              │ Track re-parses (default 1)│
│ inserted_at              │ utc_datetime_usec    │                            │
│ updated_at               │ utc_datetime_usec    │                            │
├──────────────────────────┴──────────────────────┴────────────────────────────┤
│ UNIQUE INDEX: (session_id, law_name)                                         │
│ INDEX: session_id                                                            │
│ INDEX: (session_id, group)                                                   │
│ INDEX: (session_id, status)                                                  │
└──────────────────────────────────────────────────────────────────────────────┘
```

### Table 2: `cascade_affected_laws` (Cascade Todo List) - REVISED

Tracks laws that need updating because they're referenced by parsed laws.
**Key insight**: Multiple source laws may point to the same affected law - deduplicate by `affected_law`.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         cascade_affected_laws                                │
├──────────────────────────┬──────────────────────┬────────────────────────────┤
│ Column                   │ Type                 │ Notes                      │
├──────────────────────────┼──────────────────────┼────────────────────────────┤
│ id                       │ uuid (PK)            │                            │
│ session_id               │ string               │ -> scrape_sessions.session_id │
│ affected_law             │ string               │ Law that needs updating    │
│ update_type              │ atom                 │ :reparse, :enacting_link   │
│ status                   │ atom                 │ :pending, :processed       │
│ source_laws              │ {:array, :string}    │ Which laws triggered this (audit) │
│ inserted_at              │ utc_datetime_usec    │                            │
│ updated_at               │ utc_datetime_usec    │                            │
├──────────────────────────┴──────────────────────┴────────────────────────────┤
│ UNIQUE INDEX: (session_id, affected_law)                                     │
│ INDEX: session_id                                                            │
│ INDEX: (session_id, status)                                                  │
│ INDEX: (session_id, update_type)                                             │
└──────────────────────────────────────────────────────────────────────────────┘
```

**Update Types:**
- `:reparse` - Law was amended/rescinded, needs full re-scrape from legislation.gov.uk
- `:enacting_link` - Law is a parent (enacted_by), just needs `enacting` array updated (no scrape)

**Upsert Behavior:**
When adding an affected law that already exists for this session:
- Append new source_law to `source_laws` array (for audit trail)
- Keep existing `update_type` (or upgrade to `:reparse` if was `:enacting_link`)

### Data Flow

```
                                 SCRAPE SESSION FLOW
                                 
1. Scrape new laws → scrape_session_records (status: pending)
                     + JSON files (backwards compat)
                            ↓
2. User reviews/parses → scrape_session_records (status: parsed)
                            ↓
3. User confirms     → scrape_session_records (status: confirmed)
                     → uk_lrt (persisted)
                     → cascade_affected_laws (upsert: one row per affected_law)
                            ↓
4. Cascade update    → cascade_affected_laws (status: pending → processed)
   - :reparse        → scrape from legislation.gov.uk, update uk_lrt
   - :enacting_link  → just update enacting array in uk_lrt
```

### Relationship Diagram

```
┌─────────────────────┐         ┌─────────────────────────────┐
│   scrape_sessions   │ 1 ───── │   scrape_session_records    │
├─────────────────────┤   many  ├─────────────────────────────┤
│ session_id (string) │◄────────│ session_id                  │
│ status              │         │ law_name                    │
│ group1_count, ...   │         │ group, status, parsed_data  │
└─────────────────────┘         └─────────────────────────────┘
          │
          │ 1
          │
          │ many
          ▼
┌─────────────────────────────┐
│   cascade_affected_laws     │
├─────────────────────────────┤
│ session_id                  │
│ affected_law (UNIQUE w/     │
│              session_id)    │
│ update_type                 │
│ status                      │
│ source_laws[] (audit)       │
└─────────────────────────────┘
```

### What Current JSON Files Map To

| JSON File / Field             | New DB Table / Query                    |
|-------------------------------|-----------------------------------------|
| `inc_w_si.json` records       | `scrape_session_records` (group1)       |
| `inc_wo_si.json` records      | `scrape_session_records` (group2)       |
| `exc.json` records            | `scrape_session_records` (group3)       |
| `affected_laws.json` entries  | `cascade_affected_laws` rows            |
| `all_amending` array          | Query: update_type = :reparse           |
| `all_rescinding` array        | Query: update_type = :reparse           |
| `all_enacting_parents` array  | Query: update_type = :enacting_link     |

### Key Benefits

1. **Natural deduplication** - UNIQUE on `(session_id, affected_law)` prevents reparse duplication
2. **Efficient queries** - Indexed lookups vs JSON file reads
3. **Status tracking** - Know which cascade updates are pending vs done
4. **Atomic operations** - No file corruption risk
5. **Audit trail** - `source_laws` array tracks which parsed laws triggered the cascade

**Ended**: 2026-01-21 15:37 UTC
